{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isntqCH0_EEb",
        "outputId": "d59404a4-5115-4d96-f032-23a446999780"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'omp' from 'mpi4py' (c:\\Users\\deven\\anaconda3\\lib\\site-packages\\mpi4py\\__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_41672\\4281765004.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmpi4py\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0momp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#all algos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name 'omp' from 'mpi4py' (c:\\Users\\deven\\anaconda3\\lib\\site-packages\\mpi4py\\__init__.py)"
          ]
        }
      ],
      "source": [
        "#exp 1\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def parallel_dfs(graph, start_node):\n",
        "    comm = MPI.COMM_WORLD\n",
        "    rank = comm.Get_rank()\n",
        "    size = comm.Get_size()\n",
        "\n",
        "    visited = set()\n",
        "    stack = [start_node]\n",
        "    result = []\n",
        "\n",
        "    while stack:\n",
        "        node = stack.pop()\n",
        "        if node not in visited:\n",
        "            visited.add(node)\n",
        "            result.append(node)\n",
        "            neighbors = graph[node]\n",
        "\n",
        "            # Distribute the neighbors across processes\n",
        "            chunk_size = (len(neighbors) + size - 1) // size\n",
        "            chunk = neighbors[rank * chunk_size:(rank + 1) * chunk_size]\n",
        "\n",
        "            for neighbor in chunk:\n",
        "                if neighbor not in visited:\n",
        "                    stack.append(neighbor)\n",
        "\n",
        "        # Gather results from all processes\n",
        "        result = comm.gather(result, root=0)\n",
        "\n",
        "        if rank == 0:\n",
        "            result = [node for sublist in result for node in sublist]\n",
        "\n",
        "    return result\n",
        "\n",
        "def parallel_bfs(graph, start_node):\n",
        "    comm = MPI.COMM_WORLD\n",
        "    rank = comm.Get_rank()\n",
        "    size = comm.Get_size()\n",
        "\n",
        "    visited = set()\n",
        "    queue = [start_node]\n",
        "    result = []\n",
        "\n",
        "    while queue:\n",
        "        node = queue.pop(0)\n",
        "        if node not in visited:\n",
        "            visited.add(node)\n",
        "            result.append(node)\n",
        "            neighbors = graph[node]\n",
        "\n",
        "            # Distribute the neighbors across processes\n",
        "            chunk_size = (len(neighbors) + size - 1) // size\n",
        "            chunk = neighbors[rank * chunk_size:(rank + 1) * chunk_size]\n",
        "\n",
        "            for neighbor in chunk:\n",
        "                if neighbor not in visited:\n",
        "                    queue.append(neighbor)\n",
        "\n",
        "        # Gather results from all processes\n",
        "        result = comm.gather(result, root=0)\n",
        "\n",
        "        if rank == 0:\n",
        "            result = [node for sublist in result for node in sublist]\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the graph for DFS and BFS\n",
        "graph = {\n",
        "    'A': ['B', 'C'],\n",
        "    'B': ['D', 'E'],\n",
        "    'C': ['F'],\n",
        "    'D': [],\n",
        "    'E': ['F'],\n",
        "    'F': []\n",
        "}\n",
        "\n",
        "# Perform parallel DFS\n",
        "dfs_result = parallel_dfs(graph, 'A')\n",
        "print(\"DFS Result:\", dfs_result)\n",
        "\n",
        "# Perform parallel BFS\n",
        "bfs_result = parallel_bfs(graph, 'A')\n",
        "print(\"BFS Result:\", bfs_result)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47DuK39OGduB",
        "outputId": "ddd6459a-2a37-4733-f3b4-ad4fe1bffea3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mpi4py in c:\\users\\deven\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (3.1.4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
            "[notice] To update, run: C:\\Users\\deven\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# exp 2\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "def parallel_bubble_sort(data):\n",
        "    comm = MPI.COMM_WORLD\n",
        "    rank = comm.Get_rank()\n",
        "    size = comm.Get_size()\n",
        "\n",
        "    n = len(data)\n",
        "    local_data = data[rank * (n // size):(rank + 1) * (n // size)]\n",
        "    sorted_data = comm.gather(sorted(local_data), root=0)\n",
        "\n",
        "    if rank == 0:\n",
        "        sorted_data = [item for sublist in sorted_data for item in sublist]\n",
        "        # Perform final bubble sort on root process\n",
        "        for i in range(n):\n",
        "            for j in range(n - i - 1):\n",
        "                if sorted_data[j] > sorted_data[j + 1]:\n",
        "                    sorted_data[j], sorted_data[j + 1] = sorted_data[j + 1], sorted_data[j]\n",
        "\n",
        "    return sorted_data\n",
        "\n",
        "def parallel_merge_sort(data):\n",
        "    comm = MPI.COMM_WORLD\n",
        "    rank = comm.Get_rank()\n",
        "    size = comm.Get_size()\n",
        "\n",
        "    n = len(data)\n",
        "\n",
        "    def merge_sort(arr):\n",
        "        if len(arr) <= 1:\n",
        "            return arr\n",
        "\n",
        "        mid = len(arr) // 2\n",
        "        left = arr[:mid]\n",
        "        right = arr[mid:]\n",
        "\n",
        "        left = merge_sort(left)\n",
        "        right = merge_sort(right)\n",
        "\n",
        "        return merge(left, right)\n",
        "\n",
        "    def merge(left, right):\n",
        "        merged = []\n",
        "        i, j = 0, 0\n",
        "\n",
        "        while i < len(left) and j < len(right):\n",
        "            if left[i] <= right[j]:\n",
        "                merged.append(left[i])\n",
        "                i += 1\n",
        "            else:\n",
        "                merged.append(right[j])\n",
        "                j += 1\n",
        "\n",
        "        while i < len(left):\n",
        "            merged.append(left[i])\n",
        "            i += 1\n",
        "\n",
        "        while j < len(right):\n",
        "            merged.append(right[j])\n",
        "            j += 1\n",
        "\n",
        "        return merged\n",
        "\n",
        "    local_data = data[rank * (n // size):(rank + 1) * (n // size)]\n",
        "    local_sorted_data = merge_sort(local_data)\n",
        "    sorted_data = comm.gather(local_sorted_data, root=0)\n",
        "\n",
        "    if rank == 0:\n",
        "        sorted_data = merge_sort([item for sublist in sorted_data for item in sublist])\n",
        "\n",
        "    return sorted_data\n",
        "\n",
        "# Perform parallel bubble sort\n",
        "data = [7, 3, 1, 5, 2, 4, 6]\n",
        "bubble_sorted_data = parallel_bubble_sort(data)\n",
        "print(\"Bubble Sort Result:\", bubble_sorted_data)\n",
        "\n",
        "# Perform parallel merge sort\n",
        "merge_sorted_data = parallel_merge_sort(data)\n",
        "print(\"Merge Sort Result:\", merge_sorted_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEfCrzmjHlSD",
        "outputId": "495c996f-c537-41f5-fb5d-cfbb1ea72317"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Minimum: 1\n",
            "Maximum: 7\n",
            "Sum: 28\n",
            "Average: 4.0\n"
          ]
        }
      ],
      "source": [
        "# exp 3\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "def parallel_reduction_min_max_sum_avg(data):\n",
        "    comm = MPI.COMM_WORLD\n",
        "    rank = comm.Get_rank()\n",
        "    size = comm.Get_size()\n",
        "\n",
        "    # Scatter the data across processes\n",
        "    local_data = np.array_split(data, size)[rank]\n",
        "\n",
        "    # Perform local reduction to find local min, max, sum, and count\n",
        "    local_min = np.min(local_data)\n",
        "    local_max = np.max(local_data)\n",
        "    local_sum = np.sum(local_data)\n",
        "    local_count = len(local_data)\n",
        "\n",
        "    # Perform global reduction using MPI reduce operations\n",
        "    global_min = comm.reduce(local_min, op=MPI.MIN, root=0)\n",
        "    global_max = comm.reduce(local_max, op=MPI.MAX, root=0)\n",
        "    global_sum = comm.reduce(local_sum, op=MPI.SUM, root=0)\n",
        "    global_count = comm.reduce(local_count, op=MPI.SUM, root=0)\n",
        "\n",
        "    if rank == 0:\n",
        "        global_avg = global_sum / global_count\n",
        "        return global_min, global_max, global_sum, global_avg\n",
        "\n",
        "    return None\n",
        "\n",
        "# Example usage\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data = np.array([7, 3, 1, 5, 2, 4, 6])\n",
        "\n",
        "    min_val, max_val, sum_val, avg_val = parallel_reduction_min_max_sum_avg(data)\n",
        "\n",
        "    if MPI.COMM_WORLD.Get_rank() == 0:\n",
        "        print(\"Minimum:\", min_val)\n",
        "        print(\"Maximum:\", max_val)\n",
        "        print(\"Sum:\", sum_val)\n",
        "        print(\"Average:\", avg_val)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "7fd87c73997dbabba7ad20bd38af675ca9f404c91197ff0669a923590f9df330"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
